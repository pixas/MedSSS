{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time \n",
    "from typing import Any, Callable, Union, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from Evol_Instruct.MCTS.utils import extract_template, action_stop_words_mapping\n",
    "from Evol_Instruct.models.vllm_support import get_vllm_model, vllm_clean_generate\n",
    "from Evol_Instruct.prompts.prompt_template import mcts_prompts\n",
    "from Evol_Instruct import client, logger\n",
    "\n",
    "import pickle\n",
    "import pdb\n",
    "from Evol_Instruct.MCTS.tree import MCTS, MCTSConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-11 10:53:11 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='/mnt/petrelfs/jiangshuyang.p/models/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='/mnt/petrelfs/jiangshuyang.p/models/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mnt/petrelfs/jiangshuyang.p/models/Meta-Llama-3-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 11-11 10:53:15 model_runner.py:1014] Starting to load model /mnt/petrelfs/jiangshuyang.p/models/Meta-Llama-3-8B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fedad3536dcb4f36910b36ae61f72639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-11 10:53:38 model_runner.py:1025] Loading model weights took 14.9595 GB\n",
      "INFO 11-11 10:53:40 gpu_executor.py:122] # GPU blocks: 27870, # CPU blocks: 2048\n",
      "INFO 11-11 10:53:41 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-11 10:53:41 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-11 10:54:02 model_runner.py:1456] Graph capturing finished in 21 secs.\n",
      "2024-11-11 10:54:02,641 - 71414734.py[line:6] - INFO - {'id': 'cmexam_2', 'conversations': [{'from': 'human', 'value': \"A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation?\\nA. Abnormal migration of ventral pancreatic bud\\nB. Complete failure of proximal duodenum to recanalize\\nC. Abnormal hypertrophy of the pylorus\\nD. Failure of lateral body folds to move ventrally and fuse in the midline\"}, {'from': 'gpt', 'value': 'The correct answer to this question is actually C, Abnormal hypertrophy of the pylorus, which leads to a condition known as pyloric stenosis. This condition is characterized by the overgrowth (hypertrophy) and hyperplasia of the muscular layers of the pylorus, which can cause obstruction of the gastric outlet. The clinical symptoms match: a previously well infant becoming fussy, regurgitating feeds, and yellow vomit indicative of bile, which suggests an obstruction distal to the bile duct entrance (i.e., at the pylorus). Option A relates to pancreas development and would not typically result in these symptoms. Option B, failure of recanalization of the duodenum, presents earlier in life, generally in the first week. Option D concerns the development of the body wall and would likely result in more complex congenital abnormalities. Thus, the symptoms described are classic for pyloric stenosis, making C the correct choice. The selection of option A appears to be an error.'}], 'answer_idx': 'A', 'answer': 'Abnormal migration of ventral pancreatic bud'}\n",
      "2024-11-11 10:54:02,643 - 71414734.py[line:7] - INFO - Correct answer: A\n"
     ]
    }
   ],
   "source": [
    "model, lora_request = get_vllm_model(\n",
    "    model_path='/mnt/petrelfs/jiangshuyang.p/models/Meta-Llama-3-8B-Instruct',\n",
    ")\n",
    "data = client.read(\"s3://syj_test/datasets/medical_train/mmed_en_train.json\")\n",
    "item = data[2]\n",
    "logger.info(item)\n",
    "logger.info(f\"Correct answer: {item['answer_idx']}\")\n",
    "\n",
    "# # print(node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/mnt/petrelfs/jiangshuyang.p/repo/new/WizardLM/Evol_Instruct/MCTS/tree.py\u001b[0m(548)\u001b[0;36mexpand_node\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    546 \u001b[0;31m                                              **sampling_params) \n",
      "\u001b[0m\u001b[0;32m    547 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 548 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    549 \u001b[0;31m            \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreasoning_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    550 \u001b[0;31m            \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<steps>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# timing\u001b[39;00m\n\u001b[1;32m      5\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime cost: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repo/new/WizardLM/Evol_Instruct/MCTS/tree.py:395\u001b[0m, in \u001b[0;36mMCTS.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m iter_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_terminated():\n\u001b[0;32m--> 395\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_one_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43miter_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    397\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTerminate due to -1 return code\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repo/new/WizardLM/Evol_Instruct/MCTS/tree.py:378\u001b[0m, in \u001b[0;36mMCTS.run_one_iter\u001b[0;34m(self, iter_time)\u001b[0m\n\u001b[1;32m    375\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout(node, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_func_simulation)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# pdb.set_trace()\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirect_infer_simulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# logger.debug(f\"Choose to rollout due to leaf node not visited, Value: {value}\")\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mback_propagate(node,value)\n",
      "File \u001b[0;32m~/repo/new/WizardLM/Evol_Instruct/MCTS/tree.py:573\u001b[0m, in \u001b[0;36mMCTS.rollout\u001b[0;34m(self, node, simu_func, simulation)\u001b[0m\n\u001b[1;32m    570\u001b[0m         value \u001b[38;5;241m=\u001b[39m simu_func(node)\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value \n\u001b[0;32m--> 573\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43msimu_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/repo/new/WizardLM/Evol_Instruct/MCTS/tree.py:268\u001b[0m, in \u001b[0;36mMCTS.direct_infer_simulation\u001b[0;34m(self, node, simulation_times)\u001b[0m\n\u001b[1;32m    266\u001b[0m prompt \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mbuild_simu_prompt()\n\u001b[1;32m    267\u001b[0m simulation_times \u001b[38;5;241m=\u001b[39m simulation_times \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (node\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 268\u001b[0m answers \u001b[38;5;241m=\u001b[39m \u001b[43mvllm_clean_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimulation_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m text \u001b[38;5;241m=\u001b[39m answers[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m predict_idx \u001b[38;5;241m=\u001b[39m [extract_template(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m text]\n",
      "File \u001b[0;32m~/repo/new/WizardLM/Evol_Instruct/models/vllm_support.py:115\u001b[0m, in \u001b[0;36mvllm_clean_generate\u001b[0;34m(model, prompts, system, wrap_chat, lora_request, **sampling_params)\u001b[0m\n\u001b[1;32m    113\u001b[0m max_tokens \u001b[38;5;241m=\u001b[39m sampling_params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4096\u001b[39m)\n\u001b[1;32m    114\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(temperature\u001b[38;5;241m=\u001b[39mtemperature, top_p\u001b[38;5;241m=\u001b[39mtop_p, max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msampling_params)\n\u001b[0;32m--> 115\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m generated_texts \u001b[38;5;241m=\u001b[39m [[x\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_outputs\u001b[38;5;241m.\u001b[39moutputs] \u001b[38;5;28;01mfor\u001b[39;00m all_outputs \u001b[38;5;129;01min\u001b[39;00m output]\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_texts\n",
      "File \u001b[0;32m~/miniconda3/envs/aug_med/lib/python3.10/site-packages/vllm/utils.py:1047\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1042\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1043\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1044\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m         )\n\u001b[0;32m-> 1047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aug_med/lib/python3.10/site-packages/vllm/entrypoints/llm.py:388\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[0m\n\u001b[1;32m    378\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams()\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    381\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    382\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m     guided_options\u001b[38;5;241m=\u001b[39mguided_options_request,\n\u001b[1;32m    386\u001b[0m     priority\u001b[38;5;241m=\u001b[39mpriority)\n\u001b[0;32m--> 388\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMEngine\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/miniconda3/envs/aug_med/lib/python3.10/site-packages/vllm/entrypoints/llm.py:877\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    875\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 877\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    879\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/miniconda3/envs/aug_med/lib/python3.10/site-packages/vllm/engine/llm_engine.py:1264\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_async_output_proc:\n\u001b[1;32m   1261\u001b[0m     execute_model_req\u001b[38;5;241m.\u001b[39masync_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_callbacks[\n\u001b[1;32m   1262\u001b[0m         virtual_engine]\n\u001b[0;32m-> 1264\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;66;03m# We need to do this here so that last step's sampled_token_ids can\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;66;03m# be passed to the next iteration for PP.\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39mis_multi_step:\n",
      "File \u001b[0;32m~/miniconda3/envs/aug_med/lib/python3.10/site-packages/vllm/executor/gpu_executor.py:130\u001b[0m, in \u001b[0;36mGPUExecutor.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_model\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m, execute_model_req: ExecuteModelRequest\n\u001b[1;32m    129\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[List[Union[SamplerOutput, PoolerOutput]]]:\n\u001b[0;32m--> 130\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/aug_med/lib/python3.10/site-packages/vllm/worker/worker_base.py:303\u001b[0m, in \u001b[0;36mLocalOrDistributedWorkerBase.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Executes at least one model step on the given sequences, unless no\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03msequences are provided.\"\"\"\u001b[39;00m\n\u001b[1;32m    301\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 303\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/aug_med/lib/python3.10/site-packages/vllm/worker/worker_base.py:291\u001b[0m, in \u001b[0;36mLocalOrDistributedWorkerBase.prepare_input\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    289\u001b[0m             broadcast_tensor_dict({}, src\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_driver_input_and_broadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_worker_input_from_broadcast()\n",
      "File \u001b[0;32m~/miniconda3/envs/aug_med/lib/python3.10/site-packages/vllm/worker/worker_base.py:253\u001b[0m, in \u001b[0;36mLocalOrDistributedWorkerBase._get_driver_input_and_broadcast\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_driver_worker\n\u001b[1;32m    250\u001b[0m worker_input: WorkerInput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_worker_input(\n\u001b[1;32m    251\u001b[0m     execute_model_req\u001b[38;5;241m=\u001b[39mexecute_model_req)\n\u001b[1;32m    252\u001b[0m model_input: ModelRunnerInputBase \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_model_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinished_requests_ids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    258\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m extract_previous_hidden_states(execute_model_req)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_metadata_broadcast:\n",
      "File \u001b[0;32m~/miniconda3/envs/aug_med/lib/python3.10/site-packages/vllm/worker/model_runner.py:1519\u001b[0m, in \u001b[0;36mModelRunner.prepare_model_input\u001b[0;34m(self, seq_group_metadata_list, virtual_engine, finished_requests_ids)\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_model_input\u001b[39m(\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1502\u001b[0m     seq_group_metadata_list: List[SequenceGroupMetadata],\n\u001b[1;32m   1503\u001b[0m     virtual_engine: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1504\u001b[0m     finished_requests_ids: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1505\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelInputForGPUWithSamplingMetadata:\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prepare the model input based on a given sequence group, including\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;124;03m    metadata for the sampling step.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;124;03m    If cuda graph is required, this API automatically pads inputs.\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1519\u001b[0m     model_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_model_input_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinished_requests_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m   1522\u001b[0m         \u001b[38;5;66;03m# Sampling metadata is only required for the final pp group\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m         generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_generators(finished_requests_ids)\n",
      "File \u001b[0;32m~/miniconda3/envs/aug_med/lib/python3.10/site-packages/vllm/worker/model_runner.py:1145\u001b[0m, in \u001b[0;36mGPUModelRunnerBase._prepare_model_input_tensors\u001b[0;34m(self, seq_group_metadata_list, finished_requests_ids)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     builder\u001b[38;5;241m.\u001b[39madd_seq_group(seq_group_metadata)\n\u001b[1;32m   1143\u001b[0m builder\u001b[38;5;241m.\u001b[39mreset_cached_inter_data()\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aug_med/lib/python3.10/site-packages/vllm/worker/model_runner.py:818\u001b[0m, in \u001b[0;36mModelInputForGPUBuilder.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    816\u001b[0m     input_tokens\u001b[38;5;241m.\u001b[39mextend(itertools\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m0\u001b[39m, cuda_graph_pad_size))\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 818\u001b[0m input_tokens_tensor \u001b[38;5;241m=\u001b[39m \u001b[43masync_tensor_h2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mrope_input_positions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/aug_med/lib/python3.10/site-packages/vllm/utils.py:856\u001b[0m, in \u001b[0;36masync_tensor_h2d\u001b[0;34m(data, dtype, target_device, pin_memory)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Asynchronously create a tensor and copy it from host to device.\"\"\"\u001b[39;00m\n\u001b[1;32m    855\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data, dtype\u001b[38;5;241m=\u001b[39mdtype, pin_memory\u001b[38;5;241m=\u001b[39mpin_memory, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 856\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = client.read(\"/mnt/petrelfs/jiangshuyang.p/repo/new/WizardLM/Evol_Instruct/config/trail1.json\")\n",
    "config = MCTSConfig(config)\n",
    "tree = MCTS(item, model, model.get_tokenizer(), config=config)\n",
    "# timing\n",
    "start = time.time()\n",
    "root = tree.run()\n",
    "end = time.time()\n",
    "\n",
    "logger.info(f\"Time cost: {end - start}\")\n",
    "# with open(\"datas/tree2.pkl\", 'wb') as file:\n",
    "#     pickle.dump(root, file)\n",
    "# with open(\"datas/tree2.pkl\", 'rb') as file:\n",
    "#     node = pickle.load(file)\n",
    "# print_node(root)\n",
    "# correct_path = root.obtain_correct_leaves()\n",
    "node, reasoning_step = tree.inference()\n",
    "print(reasoning_step)\n",
    "print(\"*\" * 80)\n",
    "print(node.get_trajectory())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
